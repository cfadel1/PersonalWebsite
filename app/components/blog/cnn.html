<div class="body-wrapper-indent">
    <div class="content-wrapper">
        <div class="outer-box">
            <div class="cnn-blog">
                <h1>Convolutional Neural Networks</h1>
                <div class="date-written">May 19, 2017</div>
                <div class="quarter-clear"></div>
                <h2>What is a convolutional neural network?</h2>
                <div>
                    There are many available machine learning models one of the most prominent ones is the Neural Network. When we use more than one or two neural networks then that’s what we call Deep Learning. It is a subset of machine learning that has out performed almost every other type of model. Before going more in depth, let’s answer what are convolutional neural networks and why are they so important? Convolutional Neural Networks are also known as CNNs or as ConvNets have proven to be very effective in areas such as image recognition and classification. A good example of CNN in action is Google’s famous <a class="blog-link" target="_blank" href="https://quickdraw.withgoogle.com/">QuickDraw .</a>
                </div>

                <div class="half-clear"></div>

                <h2>What is TensorFlow?</h2>
                <div>
                    TensorFlow is an open source library for machine learning developed by Google. The goal of TensorFlow is to build and train neural networks that can be used to detect patterns and correlations. The name TensorFlow is derived from the word “Tensor” representing the multidimensional arrays that the neural network performs operations on.
                </div>

                <div class="half-clear"></div>

                <div>
                    As you can see after playing the game, the neural net might or might not recognize some of your drawings, but the interesting thing is the variation of ways you could have drawn a certain thing. Neural Networks are trained on a large set of models to learn all kinds of aspects and details about it so they could identify it from any angle. CNN works by studying an image by moving multiple filters across the input image. This means each individual filter will be re-used for recognizing new and different patterns the previous filter did not test. A trained network has many layers, below is an image of the different layers in a neural network.
                </div>

                <div class="clear"></div>

                <div class="convolutionalPic"></div>
                <div class="figure-source">Figure 1: Max Pooling. Source [1]</div>

                <div class="clear"></div>

                <h2>Input Image</h2>
                <div>
                    The drawback of convolutional neural network is the amount of data required to train it especially as the data is not always available by request. We will go through the step of inputting one image into a neural network, but in its application, the CNN should be trained with 1000 images. All images however will go through the same process. With one image, the CNN can be trained to up to 13% accuracy, with two images, the accuracy can increase to around 15% but with a 1000 images and more the accuracy can reach up to over 97 %. These numbers are estimations and can vary by Neural Network.
                </div>

                <div class="half-clear"></div>

                <h2>Convolutional Layer 1</h2>
                <div>
                    I will be describing the steps that are processed in the first convolutional layer.
                    <div class="quarter-clear"></div>
                    <ul>
                        <li> Depending on the convolutional layer there are a various set of filters that are used to detect various aspects of an image. The number of filter can also differ and they’re also known as the hyper parameters for our layers. The decision on how the number of hyper parameter required is its own field of study, but there are of course hyper parameter optimization methods out there. Different filters can be used to detect different features of an image, its shape, its edges, its colors and more. Therefore, a filter can also be called a kernel or even a feature detector. The higher the number of filter the more the features we can extract and the better our network will become.</li>
                        <li> Each one of the filters strides through the image and keeps track of the result. A stride is the movement of a filter over the inputted image in either the horizontally axis or the vertical axis. Let’s consider a 5x5 picture and a 3x3 filter.</li>
                        <li> For each position, the dot product is being calculated between the filter and the image pixels under the filter. The output of the dot product gives us a “Convolved Feature” or also called an “Activation Map” or even a “Feature Map”. In the gif above, the orange matrix is the filter that is striding horizontally then vertically through the image (green matrix) and for every position, the element wise multiplication is computed between both matrices.</li>
                    </ul>

                    <div class="half-clear"></div>

                    <div class="featuremapPic"></div>
                    <div class="figure-source">Figure 2: The Convolution operation. Source [2]</div>

                    <div class="half-clear"></div>

                    <ul>
                        <li> When the filter reaches the end of the image, the image gets padded with zeros. This means, all the sections that were not detected or evaluated by the filter will get a value of zero. This is done to preserve the initial size of the image.</li>
                        <li> The output, the convolved feature can be passed through a Rectified Linear Unit (ReLU). A ReLu is a non-linear operation that takes the values from the matrix and ensures that they are all positive (it rectifies them). It is an element wise operation (applied per pixel) and replaces all negative pixel values in the feature map by zero.</li>
                        <li> One image can have many Feature Maps. This will require a lot of space so a technique for down-sampling the images will be used. The method is called max-pooling. Max-pooling goes from left to right through the output with a 2x2 filter and only keeps the largest value of those pixels. This will half the resolution of the input image, e.g. 28x28 to 14x14 pixels. There are other types of spatial pooling such as average or sum pooling, to learn more about them check out the following <a class="blog-link" href="http://cs231n.github.io/convolutional-networks/#pool"></a>page.</li>
                        <li> Output : 16 new images (1 feature map) of 14x14</li>

                    </ul>
                    <div class="quarter-clear"></div>
                    <u><b>Note:</b></u> The depth of a convolutional layer is known as the number of filters that we use for the convolution operation.
                </div>

                <div class="clear"></div>

                <div class="maxPoolingPic"></div>
                <div class="figure-source">Figure 3: Max Pooling. Source [3]</div>

                <div class="clear"></div>

                <h2>Convolutional Layer 2</h2>
                <div>
                    The output of the first convolutional layer will become the input of the second convolutional layer. The purpose of each one of the convolutional layers is to extract features from the input image while preserving the relationship between pixels. A neural network’s accuracy can increase by increasing the number of convolutional layers. This convolution layer is much more concentrated than the first one and only cares about the sections that are important
                    <div class="quarter-clear"></div>
                    <ul>
                        <li> In this case, the inputs are the 16 new images outputted from the first convolutional layer</li>
                        <li> We need a separate filter for each one of those input channels.</li>
                        <li> A channel is a term used to refers to a certain component of an image. A colored image will have 3 channels (rgb). You can imagine them as three 2d-matrices stacked over each other (one for each color), each having pixel values in the range 0 to 255.</li>
                        <li> Now let’s consider a 36-output channel. As mentioned above, there is not precise number for these hyper parameters.</li>
                        <li> Wanting 36 output channels will require us to have 16 x 36 = 576 filters for this convolutional layer. </li>
                        <li> It will use the same process as in the first convolutional layer to evaluate these pictures but with the new filters.</li>
                        <li> Output: 36 new images of 7x7 pixels each.</li>
                    </ul>
                </div>

                <div class="half-clear"></div>

                <h2>Fully-Connected Layer</h2>
                <div>
                    All the convolutional layers are connected to each other. We can say that each network takes the output of the previous and enhances its knowledge on it. After going through all these layers, the fully connected layer flattens these features into a single vector which then used as an input to another fully connected layer. After going through those layers, the last layer will use these features and classifies them into classes. A class in this case is an important property that was studied by the filter, for example the shape of the object.
                </div>

                <div class="half-clear"></div>

                <h2>Backpropogation</h2>
                <div>
                    As the fully-connected layer is summing all these features, the CNN calculates the error at the output layer. Then it uses back propagation, which is a complex topic that will be briefly discussed, to calculate the gradients of the error with respect to all the weights in the network. It then uses the gradient descent to update all hyper parameter values to minimize the output error. To learn more about gradient descent and back propagation check out the <a class="blog-link" target="_blank" href="http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/">following link.</a>
                </div>

                <div class="clear"></div>

                <div class="imgclassificationpPic"></div>
                <div class="figure-source">Figure 4: Training the CNN [2]</div>

                <div class="clear"></div>

                <h2>Conclusion</h2>
                <div>
                    As shown in the picture above, the inputted picture will go through many convolutional layers where the CNN learns about specific features in the image, then it gets classified using back propagation of the fully connected layers. I encourage you to look at the reference listed below to learn more about the topic.
                </div>

                <div class="half-clear"></div>

                <h2>References</h2>
                <div class="references">
                    <ul>
                        <li> <a class="blog-link" target="_blank" href="https://github.com/llSourcell/How_to_make_a_tensorflow_image_classifier_LIVE/blob/master/demonotes.ipynb">[1] How to make a tensorflow image classifier Live By Siraj</a></li>
                        <li> <a class="blog-link" target="_blank" href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/">[2] An Intuitive Explanation of Convolutional Neural Networks By ujjwalkarn, August 11, 2016</a></li>
                        <li> <a class="blog-link" target="_blank" href="http://cs231n.github.io/convolutional-networks/">[3] CS231n Convolutional Neural Networks for Visual Recognition, Stanford</a></li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</div>
